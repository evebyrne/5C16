{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4C16 Lab 5 - Convolutional Neural Nets for Image Classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "\n",
    "import keras\n",
    "from keras import datasets\n",
    "from keras.layers import Dense, Flatten, Dropout, Activation\n",
    "from keras.layers import PReLU, LeakyReLU, Conv2D, MaxPool2D, Lambda\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from keras.models import model_from_json\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "import pickle\n",
    "import sklearn as skl\n",
    "\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some useful functions\n",
    "class PlotLossAccuracy(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.i = 0\n",
    "        self.x = []\n",
    "        self.acc = []\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        self.val_acc = []\n",
    "        self.logs = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        self.logs.append(logs)\n",
    "        self.x.append(int(self.i))\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        self.acc.append(logs.get('acc'))\n",
    "        self.val_acc.append(logs.get('val_acc'))\n",
    "        \n",
    "        self.i += 1\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        plt.figure(figsize=(16, 6))\n",
    "        plt.plot([1, 2])\n",
    "        plt.subplot(121) \n",
    "        plt.plot(self.x, self.losses, label=\"train loss\")\n",
    "        plt.plot(self.x, self.val_losses, label=\"validation loss\")\n",
    "        plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.title('Model Loss')\n",
    "        plt.legend()\n",
    "        plt.subplot(122)         \n",
    "        plt.plot(self.x, self.acc, label=\"training accuracy\")\n",
    "        plt.plot(self.x, self.val_acc, label=\"validation accuracy\")\n",
    "        plt.legend()\n",
    "        plt.ylabel('accuracy')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.title('Model Accuracy')\n",
    "        plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "        plt.show();\n",
    "        \n",
    "def save_model_to_disk():    \n",
    "    # save model and weights (don't change the filenames)\n",
    "    model_json = model.to_json()\n",
    "    with open(\"model.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(\"model.h5\")\n",
    "    print(\"Saved model to model.json and weights to model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('loading the dataset...')\n",
    "\n",
    "pkl_file = open('/home/tcd/codiad/workspace/data/cifar10-dataset.pkl', 'rb')\n",
    "dataset = pickle.load(pkl_file)\n",
    "\n",
    "print('loaded.')\n",
    "\n",
    "print('let\\'s look at some of the pictures and their ground truth labels:')\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.plot([3, 3])\n",
    "\n",
    "X = dataset['X'].astype('float32')/255\n",
    "Y = dataset['Y'].astype('float32')\n",
    "Y = keras.utils.to_categorical(Y)\n",
    "\n",
    "for i in range(0,9):\n",
    "    # pictures are 32x32x3 (width=32, height=32, 3 colour channels)\n",
    "    pic = X[i]\n",
    "\n",
    "    # Y[i] returns an array of zeros and with Y[i][classid] = 1\n",
    "    # for instance  Y[i] = [ 0 0 0 0 0 1 0 0 0 0] => classid=5 \n",
    "    #          and  Y[i] = [ 1 0 0 0 0 0 0 0 0 0] => classid=0\n",
    "    # we can get the classid by using the argmax function on the vector Y[i]\n",
    "    classid = Y[i].argmax(-1)\n",
    "\n",
    "    # getting back the name of the label for that classid\n",
    "    classname = dataset['labels'][classid]\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.imshow(pic)\n",
    "    plt.title('label: {}'.format(classname))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's split data between validation set and training set\n",
    "\n",
    "X_train, X_validation, Y_train, Y_validation = skl.model_selection.train_test_split(X, Y, test_size=.1, random_state=0)\n",
    "# print(X_train.shape[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Architecture from here \n",
    "<https://arxiv.org/pdf/1412.6806.pdf>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model below contains 2 hidden layers with 64 nodes each. \n",
    "# The activation functions for these 2 layers is the ReLU\n",
    "# The network ends with a 10 nodes layer with softmax activation\n",
    "# The first 2 hidden layers transform the original features into \n",
    "# a new feature vector of size 64.\n",
    "# The last layer essentially does the classification using multonomial regression\n",
    "# based on these new features. \n",
    "\n",
    "inputs = keras.layers.Input(shape=(32, 32, 3))\n",
    "\n",
    "# 70% with train with batch = 1000 then 4096\n",
    "# x = Conv2D(96, [3,3], activation ='relu', padding='same', strides=1)(inputs)\n",
    "# x = Conv2D(96, [3,3], activation ='relu', padding='same', strides=2)(x)\n",
    "# x = Conv2D(96, [3,3], activation ='relu', padding='same', strides=1)(x)\n",
    "# x = Conv2D(192, [3,3], activation ='relu', padding='same', strides=2)(x)\n",
    "# x = Conv2D(192, [3,3], activation ='relu', padding='same', strides=2)(x)\n",
    "# x = Conv2D(192, [3,3], activation ='relu', padding='same', strides=1)(x)\n",
    "# https://arxiv.org/pdf/1412.6806.pdf\n",
    "x = Conv2D(96, [3,3], activation ='relu', padding='same', strides=1)(inputs)\n",
    "x = Conv2D(96, [3,3], activation ='relu', padding='same', strides=1)(x)\n",
    "# x = Dropout(0.2)(x)\n",
    "x = Conv2D(96, [3,3], activation ='relu', padding='same', strides=2)(x)\n",
    "# x = Dropout(0.2)(x)\n",
    "x = Conv2D(192, [3,3], activation ='relu', padding='same', strides=1)(x)\n",
    "x = Conv2D(192, [3,3], activation ='relu', padding='same', strides=1)(x)\n",
    "# x = Dropout(0.2)(x)\n",
    "x = Conv2D(192, [3,3], activation ='relu', padding='same', strides=2)(x)\n",
    "x = Conv2D(192, [3,3], activation ='relu', padding='same', strides=1)(x)\n",
    "# x = Dropout(0.2)(x)\n",
    "x = Conv2D(192, [1,1], activation ='relu', padding='same', strides=1)(x)\n",
    "x = Conv2D(10, [1,1], activation ='relu', padding='same', strides=1)(x)\n",
    "x = keras.layers.AveragePooling2D(pool_size=2)(x)\n",
    "x = Flatten()(x)\n",
    "\n",
    "# 10 classes\n",
    "# accuracy = 10% means that it's random(1/10 classes)\n",
    "predictions = Dense(10, activation='softmax')(x)\n",
    "\n",
    "# Create the model.\n",
    "model = keras.models.Model(inputs=inputs, outputs=predictions)\n",
    "# Adam is a more stable optimiser - will pretty much always work at lr = 0.001\n",
    "# SGD is more sensitive to the learning rate\n",
    "# opt = keras.optimizers.Adam(lr=0.001, decay=1e-6)\n",
    "opt = keras.optimizers.Adam(lr=0.001)\n",
    "\n",
    "# Setup the optimisation strategy.\n",
    "model.compile(optimizer=opt,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Display a summary.\n",
    "model.summary()\n",
    "\n",
    "# Keep things sane.\n",
    "if (model.count_params() > 10000000):    \n",
    "    raise(\"Your model is unecessarily complex, scale down!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that you can evaluate this cell repeatedly to push the training of your model further.\n",
    "# You might want to reduce the value of 'num_epochs' if each evaluation starts to take too long.\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "# Create an inst\n",
    "# ance of our callback functions class, to plot our loss function and accuracy with each epoch.\n",
    "pltCallBack = PlotLossAccuracy()\n",
    "\n",
    "# Run the training.\n",
    "model.fit(X_train, Y_train,\n",
    "          batch_size=4096, epochs=num_epochs, \n",
    "          validation_data=(X_validation, Y_validation), \n",
    "          callbacks=[pltCallBack])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write model to model.json and weights to model.h5 for submission\n",
    "\n",
    "save_model_to_disk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the terminal, add these files to git and submit the lab\n",
    "# Do the following: \n",
    "#  git add lab-05/model.json lab-05/model.h5\n",
    "#  git commit -m \"Added NN model.\"\n",
    "#  git push\n",
    "#  submit-lab 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying to do tl but not working \n",
    "<https://stackoverflow.com/questions/51994344/transfer-learning-bad-accuracy>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "base_model = ResNet50(weights='imagenet', \n",
    "                      include_top=False, \n",
    "                      input_shape=(32, 32, 3))\n",
    "\n",
    "def build_finetune_model(base_model, dropout, fc_layers, num_classes):\n",
    "    for layer in base_model.layers:\n",
    "#       if hasattr(layer, 'moving_mean') and hasattr(layer, 'moving_variance'):\n",
    "#         layer.trainable = True\n",
    "#         K.eval(K.update(layer.moving_mean, K.zeros_like(layer.moving_mean)))\n",
    "#         K.eval(K.update(layer.moving_variance, K.zeros_like(layer.moving_variance)))\n",
    "#       else:\n",
    "        layer.trainable = False\n",
    "    x = base_model.output\n",
    "    x = Flatten()(x)\n",
    "    for fc in fc_layers:\n",
    "        # New FC layer, random init\n",
    "        x = Dense(fc, activation='relu')(x) \n",
    "        x = Dropout(dropout)(x)\n",
    "\n",
    "    # New softmax layer\n",
    "    predictions = Dense(num_classes, activation='softmax')(x) \n",
    "    \n",
    "    finetune_model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    return finetune_model\n",
    "\n",
    "class_list = dataset['labels']\n",
    "FC_LAYERS = [1024]\n",
    "dropout = 0.2\n",
    "\n",
    "finetune_model = build_finetune_model(base_model, \n",
    "                                      dropout=dropout, \n",
    "                                      fc_layers=FC_LAYERS, \n",
    "                                      num_classes=10)\n",
    "opt = keras.optimizers.Adam(lr=0.001)\n",
    "\n",
    "# Setup the optimisation strategy.\n",
    "finetune_model.compile(optimizer=opt,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1st layer as the lumpsum weights from\n",
    "# resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
    "# NOTE that this layer will be set below as NOT TRAINABLE, \n",
    "# i.e., use it as is\n",
    "model_base= keras.applications.resnet50.ResNet50(include_top=False,weights='imagenet', input_shape=(32,32,3))\n",
    "print(\"len layers={}\".format(len(model_base.layers)))\n",
    "# model_base.layers.pop()\n",
    "for l in model_base.layers:\n",
    "    l.trainable=False\n",
    "    if isinstance(l, keras.layers.normalization.BatchNormalization):\n",
    "        l._per_input_updates = {}\n",
    "# for l in model_base.layers[-10:]:\n",
    "#     l.trainable=True\n",
    "# model_base.BatchNorm()(training=False) \n",
    "# model.add(ResNet50(include_top = False, pooling = 'avg', weights = 'imagenet'))\n",
    "\n",
    "# 2nd layer as Dense for 2-class classification, i.e., dog or cat using SoftMax activation\n",
    "x = model_base.output\n",
    "# x = Dropout(0.2)(x)\n",
    "x = keras.layers.GlobalAveragePooling2D()(x) \n",
    "predictions = keras.layers.Dense(10, activation='softmax')(x)\n",
    "model = keras.models.Model(model_base.input, predictions)\n",
    "# model.add(Dense(10, activation = 'softmax'))\n",
    "\n",
    "# Say not to train first layer (ResNet) model as it is already trained\n",
    "# model.layers[0].trainable = False\n",
    "\n",
    "model.summary()\n",
    "\n",
    "opt = keras.optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.9)\n",
    "\n",
    "# opt = keras.optimizers.Adam(lr=0.001)\n",
    "\n",
    "# Setup the optimisation strategy.\n",
    "model.compile(optimizer=opt,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_base = keras.applications.resnet50.ResNet50(include_top=False,weights='imagenet', input_shape=(32,32,3))\n",
    "train_vectors = model_base.predict(...)\n",
    "test_vectors = model_base.predict(...)\n",
    "model_dense = ...\n",
    "model_dense.fit(train_vectors,...)\n",
    "joined = model_base(model_dense.outputs[0])\n",
    "model_joined = Model(model_base.inputs[0], joined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Question 1: design a network using a mix of convolutional and Dense layers, and achieve 60% accuracy on the server's test set\n",
    "#\n",
    "#  Do 'git commit -a -m \"model update\"' followed by 'submit-lab 5' to check your solution.\n",
    "#\n",
    "# Question 2: push your accuracy up to as high as 80%.\n",
    "#\n",
    "# You may want to test the following:\n",
    "#\n",
    "# Change the architecture: \n",
    "#    for instance you may want to add layers, \n",
    "#    change the number of units per layer, \n",
    "#    change the activation functions\n",
    "#\n",
    "# Always check on your accuray and loss graphs that that you are not overfitting. \n",
    "#\n",
    "# Remember that you can help avoiding overfitting using \n",
    "#   - Dropout [https://keras.io/layers/core/#dropout]\n",
    "#   - Regularisers (eg. L2, L1) [https://keras.io/regularizers/]\n",
    "#\n",
    "# Optimiser. You may get faster convergence using different optimiser that rmsprop (but rmsprop is not bad)\n",
    "#\n",
    "# Learning Rate. You can tune it.\n",
    "##\n",
    "# Also note that Deeper networks will require longer training times.\n",
    "#\n",
    "# Good luck!\n",
    "#\n",
    "#\n",
    "# F.A.Q.\n",
    "#\n",
    "#   I reached 61% on my validation set, but the submission didn't pass, how come? \n",
    "#     > the test set on the server is different from your validation set\n",
    "#   Can I use convolutional layers? \n",
    "#     > You must!\n",
    "#\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}