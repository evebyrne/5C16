# success of dl fueled by: 
* big data
* powerful computing platforms

# deeper networks
* generalise better
* dont neccessarily have more units (can have one layer w loads if units)
* are harder to train

# activation fxns
* linear activation fxns do nothing
* ReLu, linear prevent vanishing gradients (not tanh or sigmoid)
